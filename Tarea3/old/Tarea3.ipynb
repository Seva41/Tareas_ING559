{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea 3 - Métodos de Aprendizajes de Máquinas en Data Science\n",
    "\n",
    "### Integrantes: Sofía Álvarez, Sebastián Dinator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de librerías y datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/seva/anaconda3/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: pandas in /Users/seva/anaconda3/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: emoji in /Users/seva/anaconda3/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/seva/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/seva/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/seva/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/seva/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy pandas emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan los datos\n",
    "train_data = pd.read_csv('train_tweets.csv') # Entrenamiento\n",
    "eval_data = pd.read_csv('test_tweets.csv') # Prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar los tweets\n",
    "import emoji\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(lambda s: emoji.replace_emoji(s, ''))\n",
    "eval_data['text'] = eval_data['text'].apply(lambda s: emoji.replace_emoji(s, ''))\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = tweet.lower() # Convertir a minúsculas (dejar??)\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE) # Eliminar URLs\n",
    "    tweet = re.sub(r'@\\w+', '', tweet) # Eliminar menciones de usuario\n",
    "    tweet = re.sub(r'#\\w+', '', tweet) # Eliminar hashtags\n",
    "    tweet = re.sub(r'\\d+', '', tweet) # Eliminar números\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation)) # Eliminar puntuaciones\n",
    "    tweet = re.sub(r'\\\\r\\\\n|\\\\n|\\\\r', ' ', tweet) # Eliminar caracteres especiales\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip() # Eliminar espacios adicionales\n",
    "    # tweet = tweet.apply(lambda s: emoji.replace_emoji(s, ''))\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas en train_data: Index(['tweet_id', 'text', 'Odio'], dtype='object')\n",
      "Columnas en test_data: Index(['tweet_id', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los nombres de las columnas\n",
    "print(\"Columnas en train_data:\", train_data.columns)\n",
    "print(\"Columnas en test_data:\", eval_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              tweet_id                                               text  \\\n",
      "0  1399516036240662528  En una amistad o soy tipo: \\r\\nLo peor es que ...   \n",
      "1  1320788179721560065  QUIEN FUE LA MARACA CULIA TE VOY A MATAR PERRA...   \n",
      "2  1079889645280944129  menos mal q se recibe el año con ropa blanca j...   \n",
      "3  1369254390134145033  Cuantos INFILTRADOS extranjeros hay ahi https:...   \n",
      "4  1533854824378290176  #ENCONTRADO #GUAYAQUIL\\r\\nFecha: 06/06/22\\r\\nS...   \n",
      "\n",
      "   Odio                                       cleaned_text  \n",
      "0     0  en una amistad o soy tipo lo peor es que siemp...  \n",
      "1     1  quien fue la maraca culia te voy a matar perra...  \n",
      "2     0  menos mal q se recibe el año con ropa blanca j...  \n",
      "3     1            cuantos infiltrados extranjeros hay ahi  \n",
      "4     0  fecha sector norte autopista a la altura del p...  \n",
      "              tweet_id                                               text  \\\n",
      "0  1533854540763742209  Estoy de acuerdo que corrijas a tus hijos pero...   \n",
      "1  1277756504519725057  @danieljadue @ArquitectsPC1 Te regalo un pico ...   \n",
      "2  1529500412402757632  @Nnicolas_M @carolinawagner_ @i_krmns El paro ...   \n",
      "3  1167425893066838016  @vroteberde Estoy en la misma, hoy no laburo c...   \n",
      "4  1399515878727749632                             @beudalgoaj puta merda   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  estoy de acuerdo que corrijas a tus hijos pero...  \n",
      "1  te regalo un pico pa que lleguis de poto wn hueco  \n",
      "2  el paro de camineros no tiene nada que ver con...  \n",
      "3  estoy en la misma hoy no laburo como buen plan...  \n",
      "4                                         puta merda  \n"
     ]
    }
   ],
   "source": [
    "# Limpieza a los conjuntos de datos\n",
    "train_data['cleaned_text'] = train_data['text'].apply(clean_tweet)\n",
    "eval_data['cleaned_text'] = eval_data['text'].apply(clean_tweet)\n",
    "\n",
    "print(train_data.head())\n",
    "print(eval_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones:\n",
      " [1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train = train_data['cleaned_text']\n",
    "y_train = train_data['Odio']\n",
    "X_test = eval_data['cleaned_text']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "NB = MultinomialNB(alpha=10)\n",
    "NB.fit(X_train_vect, y_train)\n",
    "\n",
    "predicted = NB.predict(X_test_vect)\n",
    "print(\"Predicciones:\\n\", predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predicted = NB.predict(X_test_vect)\n",
    "predictions_df = pd.DataFrame(predicted, columns=['Predicción_Odio'])\n",
    "eval_data_with_predictions = eval_data.join(predictions_df)\n",
    "eval_data_with_predictions.to_csv('test_tweets_with_predictions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
